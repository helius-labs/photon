use anyhow::{Context, Result};
use photon_indexer::ingester::parser::get_compression_program_id;
use photon_indexer::ingester::typedefs::block_info::{parse_ui_confirmed_blocked, BlockInfo};
use photon_indexer::snapshot::{
    create_snapshot_from_byte_stream, load_block_stream_from_directory_adapter,
    load_byte_stream_from_directory_adapter, DirectoryAdapter,
};
use solana_client::nonblocking::rpc_client::RpcClient;
use solana_client::rpc_client::GetConfirmedSignaturesForAddress2Config;
use solana_sdk::signature::Signature;
use std::collections::HashSet;
use std::str::FromStr;
use std::sync::Arc;

/// Test utility to create a snapshot file from compression transactions found on-chain
pub async fn create_test_snapshot_from_compression_transactions(
    rpc_url: &str,
    target_slot: u64,
    snapshot_dir_path: &str,
) -> Result<String> {
    println!("Connecting to RPC: {}", rpc_url);
    let client = RpcClient::new(rpc_url.to_string());

    // Step 1: Fetch compression transaction signatures from current slot down to target slot
    let (signatures, signature_to_slot_map) =
        fetch_compression_signatures_until_slot(&client, target_slot).await?;
    println!(
        "Found {} compression transaction signatures:",
        signatures.len()
    );
    for (i, signature) in signatures.iter().enumerate() {
        println!("  {}. {}", i + 1, signature);
    }

    if signatures.is_empty() {
        return Err(anyhow::anyhow!(
            "No compression transactions found on devnet"
        ));
    }

    // Step 2: Extract unique slots from signature info (we already have this data!)
    let slots: HashSet<u64> = signatures
        .iter()
        .filter_map(|sig| signature_to_slot_map.get(sig))
        .copied()
        .collect();

    let mut slots: Vec<u64> = slots.into_iter().collect();
    slots.sort();
    println!(
        "Found {} unique slots with compression transactions:",
        slots.len()
    );
    for (i, slot) in slots.iter().enumerate() {
        println!("  {}. Slot {}", i + 1, slot);
    }

    // Step 3: Fetch blocks for these slots
    let mut blocks = Vec::new();
    for (i, slot) in slots.iter().enumerate() {
        match client
            .get_block_with_config(
                *slot,
                solana_client::rpc_config::RpcBlockConfig {
                    encoding: Some(solana_transaction_status::UiTransactionEncoding::Base64),
                    transaction_details: Some(solana_transaction_status::TransactionDetails::Full),
                    rewards: None,
                    commitment: Some(solana_sdk::commitment_config::CommitmentConfig::confirmed()),
                    max_supported_transaction_version: Some(0),
                },
            )
            .await
        {
            Ok(block) => match parse_ui_confirmed_blocked(block, *slot) {
                Ok(block_info) => {
                    let block_time = std::time::UNIX_EPOCH
                        + std::time::Duration::from_secs(block_info.metadata.block_time as u64);
                    let datetime = std::time::SystemTime::now()
                        .duration_since(block_time)
                        .map(|d| format!("{:.1} seconds ago", d.as_secs_f64()))
                        .unwrap_or_else(|_| {
                            format!("timestamp: {}", block_info.metadata.block_time)
                        });
                    println!(
                        "Successfully parsed block at slot {} ({} transactions, {}) [{}/{}]",
                        slot,
                        block_info.transactions.len(),
                        datetime,
                        i + 1,
                        slots.len()
                    );
                    blocks.push(block_info);
                }
                Err(e) => {
                    eprintln!("Failed to parse block at slot {}: {}", slot, e);
                }
            },
            Err(e) => {
                eprintln!("Failed to fetch block at slot {}: {}", slot, e);
            }
        }
    }

    if blocks.is_empty() {
        return Err(anyhow::anyhow!("No blocks could be fetched and parsed"));
    }

    println!("Successfully fetched and parsed {} blocks", blocks.len());

    // Step 4: Create snapshot from blocks
    let snapshot_dir = std::path::PathBuf::from(snapshot_dir_path);
    std::fs::create_dir_all(&snapshot_dir)?;

    let snapshot_dir_str = snapshot_dir.to_str().unwrap().to_string();
    let directory_adapter = Arc::new(DirectoryAdapter::from_local_directory(
        snapshot_dir_path.to_string(),
    ));

    // Clear any existing snapshots
    let existing_snapshots =
        photon_indexer::snapshot::get_snapshot_files_with_metadata(directory_adapter.as_ref())
            .await?;
    for snapshot in existing_snapshots {
        directory_adapter.delete_file(snapshot.file).await?;
    }

    // Sort blocks by slot to ensure proper ordering
    blocks.sort_by_key(|block| block.metadata.slot);

    // Calculate the total slot range to write everything into one file
    let first_slot = blocks
        .first()
        .map(|b| b.metadata.slot)
        .unwrap_or(target_slot + 1);
    let last_slot = blocks
        .last()
        .map(|b| b.metadata.slot)
        .unwrap_or(target_slot + 1);
    let slot_range = last_slot - first_slot + 1;

    println!(
        "Writing all blocks from slot {} to {} into one snapshot file (range: {} slots)",
        first_slot, last_slot, slot_range
    );

    // Create snapshot file directly without using update_snapshot_helper
    let snapshot_filename = format!("snapshot-{}-{}", first_slot, last_slot);
    let snapshot_path = snapshot_dir.join(&snapshot_filename);

    println!("Writing snapshot directly to: {:?}", snapshot_path);

    // Serialize all blocks directly (no version header in individual files)
    let mut snapshot_data = Vec::new();

    // Add serialized blocks only (header is added when reading multiple files)
    for block in &blocks {
        // Filter for compression transactions only
        let trimmed_block = photon_indexer::ingester::typedefs::block_info::BlockInfo {
            metadata: block.metadata.clone(),
            transactions: block
                .transactions
                .iter()
                .filter(|tx| photon_indexer::snapshot::is_compression_transaction(tx))
                .cloned()
                .collect(),
        };
        let block_bytes = bincode::serialize(&trimmed_block).unwrap();
        snapshot_data.extend(block_bytes);
    }

    // Write snapshot file directly
    let data_len = snapshot_data.len();
    std::fs::write(&snapshot_path, snapshot_data)?;
    println!(
        "Successfully wrote snapshot file: {:?} ({} bytes)",
        snapshot_path, data_len
    );

    println!(
        "Snapshot created successfully in directory: {}",
        snapshot_dir_str
    );

    // Debug: List created snapshot files
    let created_snapshots =
        photon_indexer::snapshot::get_snapshot_files_with_metadata(directory_adapter.as_ref())
            .await?;
    println!("Created {} snapshot files:", created_snapshots.len());
    for snapshot in &created_snapshots {
        println!(
            "  - {} (slots {} to {})",
            snapshot.file, snapshot.start_slot, snapshot.end_slot
        );
    }

    Ok(snapshot_dir_str)
}

/// Validate that photon can parse the generated snapshot
pub async fn validate_snapshot_parsing(snapshot_dir: &str) -> Result<Vec<BlockInfo>> {
    let directory_adapter = Arc::new(DirectoryAdapter::from_local_directory(
        snapshot_dir.to_string(),
    ));

    // Load and parse the snapshot
    let block_stream = load_block_stream_from_directory_adapter(directory_adapter.clone()).await;
    let blocks: Vec<Vec<BlockInfo>> = futures::StreamExt::collect(block_stream).await;
    let blocks: Vec<BlockInfo> = blocks.into_iter().flatten().collect();

    println!("Successfully parsed {} blocks from snapshot", blocks.len());

    // Validate that all blocks contain only compression transactions
    for (i, block) in blocks.iter().enumerate() {
        println!(
            "Block {} at slot {}: {} transactions",
            i,
            block.metadata.slot,
            block.transactions.len()
        );

        for (j, tx) in block.transactions.iter().enumerate() {
            let is_compression = photon_indexer::snapshot::is_compression_transaction(tx);
            if !is_compression {
                return Err(anyhow::anyhow!(
                    "Block {} transaction {} is not a compression transaction",
                    i,
                    j
                ));
            }
        }
    }

    println!("All transactions in snapshot are compression transactions âœ“");
    Ok(blocks)
}

/// Test round-trip: create snapshot and reload it via byte stream
pub async fn test_snapshot_roundtrip(snapshot_dir: &str) -> Result<()> {
    let source_adapter = Arc::new(DirectoryAdapter::from_local_directory(
        snapshot_dir.to_string(),
    ));

    // Create a second directory for the round-trip test
    let roundtrip_dir = std::path::PathBuf::from("target")
        .join("test_snapshots")
        .join("roundtrip");
    std::fs::create_dir_all(&roundtrip_dir)?;
    let roundtrip_dir_str = roundtrip_dir.to_str().unwrap().to_string();
    let target_adapter = Arc::new(DirectoryAdapter::from_local_directory(roundtrip_dir_str));

    // Load byte stream from source
    let byte_stream = load_byte_stream_from_directory_adapter(source_adapter.clone()).await;

    // Create snapshot from byte stream in target
    create_snapshot_from_byte_stream(byte_stream, target_adapter.as_ref()).await?;

    // Load blocks from both snapshots and compare
    let source_blocks = load_block_stream_from_directory_adapter(source_adapter).await;
    let source_blocks: Vec<Vec<BlockInfo>> = futures::StreamExt::collect(source_blocks).await;
    let source_blocks: Vec<BlockInfo> = source_blocks.into_iter().flatten().collect();

    let target_blocks = load_block_stream_from_directory_adapter(target_adapter).await;
    let target_blocks: Vec<Vec<BlockInfo>> = futures::StreamExt::collect(target_blocks).await;
    let target_blocks: Vec<BlockInfo> = target_blocks.into_iter().flatten().collect();

    if source_blocks.len() != target_blocks.len() {
        return Err(anyhow::anyhow!(
            "Block count mismatch: source={}, target={}",
            source_blocks.len(),
            target_blocks.len()
        ));
    }

    for (i, (source_block, target_block)) in
        source_blocks.iter().zip(target_blocks.iter()).enumerate()
    {
        if source_block != target_block {
            return Err(anyhow::anyhow!(
                "Block {} differs between source and target",
                i
            ));
        }
    }

    println!(
        "Round-trip test passed: {} blocks match exactly",
        source_blocks.len()
    );
    Ok(())
}

async fn fetch_compression_signatures_until_slot(
    client: &RpcClient,
    target_slot: u64,
) -> Result<(Vec<Signature>, std::collections::HashMap<Signature, u64>)> {
    let mut signatures = Vec::new();
    let mut signature_to_slot_map = std::collections::HashMap::new();
    let mut before = None;

    println!(
        "Fetching ALL compression signatures from current slot down to slot {}",
        target_slot
    );

    loop {
        let config = GetConfirmedSignaturesForAddress2Config {
            before,
            until: None,
            limit: None, // No limit - fetch as many as possible per batch
            commitment: None,
        };

        let compression_program_id =
            solana_sdk::pubkey::Pubkey::new_from_array(get_compression_program_id().to_bytes());
        println!(
            "Fetching signatures for compression program: {}",
            compression_program_id
        );
        let batch = client
            .get_signatures_for_address_with_config(&compression_program_id, config)
            .await
            .context("Failed to fetch signatures for compression program")?;

        println!("Fetched {} signatures in this batch", batch.len());

        let mut reached_target_slot = false;
        for sig_info in &batch {
            // Check if we've reached the target slot
            if sig_info.slot < target_slot {
                reached_target_slot = true;
                break;
            }

            // Skip failed transactions
            if sig_info.err.is_some() {
                continue;
            }

            let signature =
                Signature::from_str(&sig_info.signature).context("Failed to parse signature")?;
            signatures.push(signature);
            signature_to_slot_map.insert(signature, sig_info.slot);
        }

        if reached_target_slot {
            // Stop when no more signatures or reached target slot
            break;
        }

        before = batch
            .last()
            .map(|sig| Signature::from_str(&sig.signature).unwrap());
    }

    println!(
        "Found {} total compression signatures down to slot {}",
        signatures.len(),
        target_slot
    );
    Ok((signatures, signature_to_slot_map))
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    #[ignore] // Remove this to run the test
    async fn test_create_snapshot_from_compression_transactions() {
        let snapshot_dir = create_test_snapshot_from_compression_transactions(
            "https://api.devnet.solana.com",
            10, // Fetch 10 compression transactions
            "target/test_snapshots/devnet",
        )
        .await
        .expect("Failed to create test snapshot");

        let blocks = validate_snapshot_parsing(&snapshot_dir)
            .await
            .expect("Failed to validate snapshot parsing");

        assert!(!blocks.is_empty(), "Snapshot should contain blocks");

        test_snapshot_roundtrip(&snapshot_dir)
            .await
            .expect("Round-trip test failed");

        println!("Test completed successfully!");
        println!("Snapshot directory: {}", snapshot_dir);
        println!("Parsed {} blocks from snapshot", blocks.len());
    }
}
